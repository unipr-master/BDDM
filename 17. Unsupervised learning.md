## Estrazione di pattern frequenti

L’estrazione di **pattern frequenti** consente di individuare **relazioni ricorrenti** all’interno dei dati, permettendo di fare **associazioni tra esempi** e scoprire nuove connessioni. I pattern frequenti sono schemi che **appaiono frequentemente** in un insieme di dati.

Questa tecnica può essere applicata in diversi scenari, tra cui:

- Analisi del carrello della spesa (frequenti associazioni tra prodotti acquistati)
- Bioinformatica (associazione tra geni e malattie)
- Sistemi di rilevamento delle intrusioni (identificazione di attività sospette o dannose)
  
## Analisi del carrello della spesa

Un esempio tipico di estrazione di pattern frequenti è la **market basket analysis**, ovvero l’analisi del carrello della spesa. Questo processo esamina le abitudini di acquisto dei clienti trovando associazioni tra i diversi articoli inseriti nei carrelli.

Ad esempio, se i clienti acquistano il latte, qual è la probabilità che acquistino anche il pane — e quale tipo di pane — durante la stessa visita al supermercato?

## Regole di associazione

I dati delle transazioni possono essere analizzati per individuare **schemi di acquisto**, ovvero prodotti che vengono **acquistati frequentemente insieme**. Tali schemi possono essere rappresentati sotto forma di **regole di associazione**.

Per esempio, l'informazione che i clienti che acquistano un computer tendono anche ad acquistare un software antivirus può essere espressa come:

$\text{computer} \rightarrow \text{antivirus\_software} [\text{support} = 2\%, \text{confidence} = 60\%]$

- Il **supporto = 2%** indica che nel 2% di tutte le transazioni, i due articoli sono stati acquistati insieme.

- La **confidenza = 60%** indica che il 60% dei clienti che hanno acquistato un computer ha acquistato anche un antivirus.

## Supporto e confidenza

Possiamo ora definire formalmente queste due misure fondamentali.

Sia $A$ un insieme di articoli (ad esempio ${\text{computer}}$) e $B$ un altro insieme (ad esempio $\text{antivirus\_software}$).

La **regola** $A \Rightarrow B$ vale:

- con **supporto** $s$ se $s$ è la percentuale di transazioni che contengono $A \cup B$, cioè:    

$$\text{support}(A \Rightarrow B) = P(A \cup B)$$

- con **confidenza** $c$ se $c$ è la percentuale delle transazioni che contengono $A$ e che contengono anche $B$, cioè:

$$\text{confidence}(A \Rightarrow B) = P(B \mid A)$$


## Estrazione di regole forti di associazione

Le regole di associazione sono considerate **forti** se soddisfano **due condizioni**:

1. Un **valore minimo di supporto**    
2. Un **valore minimo di confidenza**

Queste soglie possono essere stabilite dagli utenti o da esperti di dominio.

### Processo in due fasi

L’estrazione di regole di associazione forti è un processo in due fasi:

1. **Individuare tutti gli insiemi di articoli frequenti**: ogni insieme deve comparire almeno con una frequenza pari al supporto minimo. Se un insieme $A$ non soddisfa la soglia, allora anche ogni suo superset $A \cup B$ non la soddisferà (es. se il latte non è frequente, nemmeno latte e caffè lo saranno).
 
2. **Generare le regole di associazione forti** a partire dagli insiemi frequenti: ogni regola $A \Rightarrow B$ generata deve soddisfare sia la soglia di supporto sia quella di confidenza.    


## Riepilogo sull’estrazione di regole di associazione

Abbiamo visto come **trovare iterativamente insiemi di articoli frequenti** e **filtrarli** usando soglie minime di supporto e confidenza. L’algoritmo più noto utilizzato per questo compito è l’**algoritmo Apriori**.

L’**Apriori property** afferma che se un insieme non supera un test, **nessuno dei suoi superset** lo supererà. Questo principio consente di **potare** lo spazio di ricerca, rendendo l’algoritmo più efficiente.

L’estrazione di regole di associazione è un esempio di **apprendimento non supervisionato**.

## Clustering

L’estrazione di regole ci ha permesso di trovare associazioni tra oggetti. Il passo successivo è vedere come **raggruppare oggetti in base a caratteristiche comuni**. Questa tecnica è nota come **clustering**.

L’obiettivo del clustering è **raggruppare oggetti simili** (come documenti, pazienti o case) in insiemi detti **cluster**, basandosi sulle loro caratteristiche. Di solito, gli oggetti sono raggruppati se sembrano appartenere alla stessa “categoria”, ma:

- **Le categorie non sono note a priori**
- Non esiste una verità oggettiva: il clustering è **non supervisionato**
- L’**assenza di etichette** consente di scoprire nuove caratteristiche comuni e pattern inattesi

Il clustering permette dunque di **scoprire strutture nei dati** che non erano conosciute in partenza, aprendo a nuove interpretazioni.

# Distorsione nel clustering

È comune affermare che i cluster siano "negli occhi di chi guarda": ad esempio, i termini “cavallo” e “auto” sono simili? Nell’esempio precedente, la distorsione del clustering era data dalla forma di ciascun oggetto, ed era evidente. Tuttavia, con un numero maggiore di caratteristiche, la distinzione può risultare meno ovvia. Si può effettuare il clustering in base alla forma, al colore o alla dimensione.

![[Pasted image 20250430203912.png|400]]

# k-means

Il clustering k-means mira a suddividere $n$ campioni in $k$ cluster, dove ciascun campione appartiene al cluster con la media più vicina, che funge da prototipo del cluster. L’algoritmo per k-means parte con un clustering “debole” che viene raffinato in più iterazioni fino a quando non avvengono più cambiamenti. Il parametro $k$ è definito dall’utente e rappresenta il numero di cluster desiderato.

## Algoritmo k-means

Si consideri un dataset di $m$ campioni bidimensionali. Sia il dataset definito come:

$\texttt{samples} = [(x_{11}, x_{12}), \ldots, (x_{m1}, x_{m2})]$

Si selezionano $k$ campioni casuali come centroidi iniziali:

$\texttt{centroids} = \texttt{pick\_random(samples, }k\texttt{)}$

Si effettua un primo clustering debole:

$\texttt{for each sample in samples:}$  
 $\texttt{cluster[sample]} = \texttt{nearest(sample, centroids)}$

Si inizializza la variabile di controllo dei cambiamenti:

$\texttt{anychange} = \texttt{True}$

$\texttt{while (anychange):}$  
 $\texttt{anychange} = \texttt{False}$  
 $\texttt{centroids} = [\mu_1, \ldots, \mu_k]$, dove $\mu_j = \texttt{mean(cluster}_j\texttt{)}$  

 $\texttt{for each sample in samples:}$  
  $\texttt{if cluster[sample]} \ne \texttt{nearest(sample, centroids):}$  
   $\texttt{cluster[sample]} = \texttt{nearest(sample, centroids)}$  
   $\texttt{anychange} = \texttt{True}$

# Convergenza del k-means


I **numeri di Stirling di seconda specie** forniscono il numero di suddivisioni di un insieme di $n$ oggetti in $k$ cluster. Si tratta di un numero grande, ma finito. La formula per calcolare questi numeri è:

$$\left\{ {n \atop k} \right\} = \frac{1}{k!} \sum_{j=1}^{k} (-1)^{k-j} \binom{k}{j} j^n$$

A ogni iterazione dell’algoritmo, si genera un nuovo clustering basato unicamente sul clustering precedente. In ciascun ciclo può accadere una delle due situazioni seguenti:

1. Il vecchio clustering è uguale a quello nuovo → l’iterazione si arresta.
2. Il nuovo clustering è diverso → la distanza tra i campioni e i centroidi è minore.

Se il vecchio clustering è uguale a quello nuovo, allora anche il clustering successivo sarà identico. Se invece il nuovo clustering è diverso dal precedente, allora il successivo avrà un costo ancora più basso.

# Selezione dei primi k punti

Il modo in cui vengono scelti i primi $k$ punti può influenzare la velocità di convergenza dell’algoritmo k-means. Il ciclo di assegnazione (ciclo interno) è lineare rispetto al numero di punti per ogni cluster, ma il numero di iterazioni (ciclo esterno) è sconosciuto. Di solito il numero di cicli è molto inferiore a $m$, ma nel caso peggiore si possono raggiungere $2^m$ cicli, con conseguente tempo esponenziale.

Un buon metodo per scegliere i primi punti consiste nel selezionare un insieme disperso di punti:

Si seleziona il primo punto in modo casuale. Si sceglie quindi il punto successivo come quello con la massima distanza minima dai punti già selezionati. Si ripete il processo fino ad avere $k$ punti.

## Selezione dei centroidi

Quando è stato selezionato un solo centroide, la scelta del successivo è semplice, poiché l’espressione si riduce a:

$\max \left( d(p_1, c_1), \ldots, d(p_m, c_1) \right)$

Supponiamo ora di aver già selezionato $s$ centroidi $c$ e di voler scegliere il prossimo tra i $m$ punti $p$. In questo caso si calcola:

$\max \left( \min \left( d(p_1, c_1), \ldots, d(p_1, c_s) \right), \ldots, \min \left( d(p_m, c_1), \ldots, d(p_m, c_s) \right) \right)$

Quando sono già stati scelti due o più centroidi, per ciascun punto si cerca innanzitutto il centroide più vicino (cioè si calcola il minimo), poi si utilizza questo valore per selezionare il punto che è più lontano da ogni centroide (cioè il massimo tra i minimi). Questo approccio permette di identificare il punto con la **massima distanza minima** da qualunque centroide già selezionato.

## Scelta del numero di cluster $k$

La scelta di in quanti cluster raggruppare i documenti dipende dal compito specifico, e non esiste un metodo automatico universalmente valido. Tuttavia, alcune misure possono aiutare nella scelta: solitamente si utilizza la **distanza media di tutti i documenti dal centroide del cluster**. Si esegue quindi il k-means per diversi valori di $k$. È importante notare che, aumentando $k$, la distanza media diminuirà sempre.

Si dovrebbe arrestare l’aumento di $k$ quando i miglioramenti diventano trascurabili: questo punto è noto come **elbow point**.

## Metodo del gomito (elbow method)

Il **metodo del gomito** è un approccio visivo per individuare approssimativamente il valore migliore di $k$. 

![[Pasted image 20250430204638.png|500]]

Quando il grafico a linee della distanza media rispetto a $k$ assume la forma di un braccio, il "gomito" del braccio rappresenta il valore di $k$ che offre il miglior compromesso tra semplicità e qualità del clustering.


