## Regressione

Il problema della regressione consiste nel **predire il valore di una variabile numerica** a partire dai valori di altre variabili. L’esempio più semplice è quello con due variabili, $x$ e $y$, dove $x$ è la variabile in input e $y$ è quella che si vuole prevedere.

## Regressione lineare

Abbiamo definito la regressione come il compito generale di predizione del valore reale di una variabile usando altre variabili come input. Il modello che vogliamo apprendere è una funzione $f$ che mappa $n$ valori reali $(x_1, ..., x_n)$ in un unico valore reale $y$, cioè:

$y = f(x_1, ..., x_n)$

Il modello appreso prende il nome di **funzione ipotesi** $h$, quindi:

$y = h(x_1, ..., x_n)$

Quando la funzione $h$ è **lineare** rispetto alle variabili di input $x_i$, il compito prende il nome di **regressione lineare**.

## Funzione lineare

Una funzione lineare ha la seguente forma:

$y = f(x_1, ..., x_n) = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b$

Quando $n = 1$, cioè si ha una sola variabile di input, la funzione rappresentata graficamente è una retta. Quando $n = 2$, la funzione rappresenta un piano lineare, e così via. Rette e piani sono tutti esempi di funzioni lineari.

![[Pasted image 20250430185557.png|500]]

## Parametri della regressione lineare

La funzione lineare che modella i nostri dati è **già definita nella forma**, ma mancano i **parametri da apprendere**. Le variabili $x_i$ sono gli input del modello e sono già noti (ad esempio i metri quadrati di una casa). I **coefficienti $w_i$**, invece, non sono noti: rappresentano quanto ciascuna variabile contribuisce al risultato finale $y$. Questi coefficienti sono anche detti **pesi** nel machine learning. Se le variabili sono state normalizzate, ad esempio in un intervallo tra 0 e 1, i pesi indicano l'importanza relativa di ciascuna variabile nella previsione.

Anche il parametro $b$, noto come **intercetta** in una retta, è sconosciuto. Rappresenta il punto in cui la retta interseca l’asse $y$ ed è detto **termine di bias** o **peso di bias** nel contesto dell’apprendimento automatico.

## Funzione di perdita

Nel machine learning, per valutare quanto bene si comporta una funzione ipotesi, si utilizza una **funzione di perdita** (_loss function_). L’idea alla base è semplice: una funzione di perdita misura quanto il modello si discosta dai dati reali.

La funzione di perdita più comune è il **mean squared error (MSE)**, cioè l’**errore quadratico medio**. Esso rappresenta la media della distanza quadrata tra i valori previsti e quelli reali nel dataset di addestramento. Viene calcolato su tutti i $m$ esempi del training set per ottenere un valore medio, e il quadrato della differenza garantisce che il risultato sia sempre **non negativo**.

La formula è la seguente:

$$\frac{1}{m} \sum_{i=1}^{m} \left(h(x_i) - y_i\right)^2$$

Dove:

- $m$ è il numero totale di esempi nel dataset (media aritmetica),
- $h(x_i)$ è il valore predetto dal modello per l’input $x_i$,
- $y_i$ è il valore reale osservato,    
- $\left(h(x_i) - y_i\right)^2$ è il quadrato della differenza, per evitare valori negativi e penalizzare errori più grandi.

Questa funzione rappresenta il criterio da minimizzare durante l’addestramento del modello: l’obiettivo dell’ottimizzazione è trovare i valori ottimali per $w_1, ..., w_n$ e $b$ che riducano al minimo l’errore quadratico medio.

## Ottimizzazione

Ora che disponiamo di uno strumento per misurare l’errore di una funzione ipotesi, l’obiettivo diventa **minimizzare tale errore**. In termini matematici, la nostra **funzione obiettivo** consiste nel trovare i valori ottimali delle variabili (cioè i pesi e il bias) che rendono l’errore il più piccolo possibile. Questo si traduce nella seguente espressione:

$$\min \frac{1}{m} \sum_{i=1}^{m} \left(h(x_i) - y_i\right)^2$$

Sostituendo la funzione ipotesi $h(x)$ con la funzione lineare $(wx_i + b)$ e utilizzando la notazione $\arg\min$, otteniamo:

$\arg\min_{w,b} \frac{1}{m} \sum_{i=1}^{m} \left((wx_i + b) - y_i\right)^2$


## Algoritmi di ottimizzazione

L’algoritmo di ottimizzazione più popolare ed efficace nel machine learning è la **discesa del gradiente** (_gradient descent_). Questo metodo si basa su un processo iterativo che si ripete finché la **funzione di perdita** non scende al di sotto di una soglia di tolleranza prefissata. Il procedimento si articola in quattro passaggi:

1. **Calcolare il gradiente**, ovvero la derivata (o pendenza) della funzione di perdita.
2. Il **segno del gradiente** indica se l’errore aumenta (positivo) o diminuisce (negativo) muovendosi verso destra. Poiché vogliamo raggiungere un minimo:

    - Se l’errore aumenta, dobbiamo **muoverci verso sinistra** (sottrarre un piccolo valore al peso).        
    - Se l’errore diminuisce, dobbiamo **muoverci verso destra** (aggiungere un piccolo valore al peso).

3. **Aggiornare la funzione ipotesi** con i nuovi pesi calcolati.    
4. **Ricalcolare l’errore**, e ripetere.

## Set di test

È importante ricordare che la funzione di perdita serve per valutare la funzione ipotesi **sul training set**, ovvero misura quanto la predizione si avvicina ai dati noti. Se la perdita è pari a 0, significa che il modello si adatta perfettamente ai dati di addestramento.

Tuttavia, questo non garantisce che il modello sia accurato con dati nuovi, mai visti prima. Questo fenomeno si chiama **overfitting**: il modello è eccessivamente specializzato sui dati di addestramento e **non generalizza bene**.

Per testare effettivamente le prestazioni del modello, bisogna usare un insieme di dati **mai visto prima dall’algoritmo**: il **testing set**.

## Suddivisione training/test

Nel processo di data mining, i dati non sono in genere già divisi tra training e testing set. È necessario effettuare una **suddivisione ragionata**, tenendo conto dei seguenti aspetti:

- Maggiore è la quantità di dati nel **training set**, migliore sarà la capacità del modello di apprendere.
- Maggiore è la quantità di dati nel **testing set**, più affidabile sarà la stima della sua accuratezza.

Un punto fondamentale è che **entrambi i set devono provenire dalla stessa distribuzione**. Ad esempio, non è corretto addestrare un modello sui prezzi delle case a Los Angeles e testarlo su case a New York. Analogamente, non si può addestrare un classificatore di immagini usando foto scattate da uno smartphone e testarlo su immagini provenienti da Google.

Una regola empirica diffusa è quella dell’**80/20**, nota anche come **principio di Pareto**:

- 80% dei dati per l’addestramento (training set)
- 20% per il test (testing set)

In presenza di **molti esempi**, può essere conveniente ridurre la dimensione del testing set per dedicare più dati all’addestramento, mantenendo comunque un campione di test statisticamente significativo.

#### E se volessimo usare tutti i dati?

Nel caso in cui si desideri utilizzare **tutti i dati sia per l’addestramento che per il test**, esistono tecniche specifiche come la **validazione incrociata (cross-validation)**, che permettono di massimizzare l’uso dei dati mantenendo una stima realistica delle prestazioni.

## Cross-validation (CV)

La **cross-validation**, o validazione incrociata, è una tecnica che consiste nel **ruotare la suddivisione tra set di addestramento e di test**, in modo che **ogni esempio venga utilizzato sia per l’addestramento che per il test**, in momenti diversi. La frazione dedicata al set di test determina il numero di rotazioni, chiamate anche **fold**.

Ad esempio, utilizzando un set di test pari al 20% (cioè $\frac{1}{5}$ del totale), si esegue una **cross-validation a 5 fold**.

![[Pasted image 20250430190926.png|600]]

### Scenari limite della cross-validation

Nel caso di una **2-fold cross-validation**, i dati vengono suddivisi in due metà:

- Nel primo fold, la prima metà viene usata per il test e la seconda per l’addestramento.
- Nel secondo fold, le metà si invertono: la prima serve per l’addestramento, la seconda per il test.

Nel caso estremo, una **m-fold cross-validation**, in cui $m$ è il numero totale di esempi nel dataset, ogni fold contiene **un solo esempio nel set di test**. Questo metodo è noto anche come **Leave-One-Out Cross-Validation (LOOCV)**.

Il procedimento è il seguente:

- Nel primo fold, il primo esempio è usato per il test, mentre gli altri $m-1$ esempi per l’addestramento.
- Nel secondo fold, il secondo esempio è usato per il test, e tutti gli altri (tranne il secondo) per l’addestramento.
	... 
- Nell’ultimo fold, l’ultimo esempio è usato per il test, e tutti i precedenti per l’addestramento.    

Alla fine della cross-validation, **l’errore del modello viene calcolato come media di tutti gli errori ottenuti nei diversi fold**. Ad esempio, nella LOOCV si calcola la media di $m$ errori.

## Funzioni non lineari

Finora abbiamo considerato metodi di regressione **basati su una funzione lineare** tra le variabili di input e l’output. Nel caso più semplice, con una sola variabile $x$, la relazione lineare è:

$y = f(x) = wx + b$

Ma cosa accade se la relazione tra $x$ e $y$ **non è lineare**? Una funzione non lineare può avere la forma:

$y = f(x) = w_1 x + w_2 x^2 + w_3 x^3 + \dots$

In questo caso, ogni potenza di $x$ ha un proprio coefficiente $w$, e la funzione diventa un **polinomio**.

### Introduzione della non-linearità tramite nuove feature

Un trucco semplice per introdurre la non-linearità è quello di **generare artificialmente nuove caratteristiche (feature)**. A partire da una variabile $x_1$, possiamo creare:

- La nuova feature $x_1^2$ e aggiungerla ai dati
- La feature $x_1^3$
- E così via, creando nuove colonne nel dataset

Tuttavia, questo approccio presenta **diversi limiti**:

- L’aggiunta delle nuove feature è **manuale**, da eseguire prima dell’addestramento.
- Non è chiaro **fino a che grado del polinomio ci si debba spingere**. Fermarsi a $x^3$? O $x^4$? O addirittura $x^{20}$?
- Non si considerano automaticamente **combinazioni tra feature**, ad esempio $x_1 x_2$ o $x_1^2 x_2^3 x_2^2$.
- Il numero di feature può crescere **esponenzialmente**, causando un’esplosione della **dimensionalità**.

## Limiti e metodi avanzati

L’approccio descritto è una soluzione ingenua per gestire funzioni non lineari, ma può diventare rapidamente inefficiente. Esistono metodi avanzati di regressione che:

- Non richiedono la generazione manuale delle feature    
- Sono progettati per gestire direttamente la **non linearità**
- Operano in spazi trasformati o impliciti (come i metodi basati su kernel)
 
Queste tecniche permettono di modellare relazioni complesse nei dati in modo più efficace e automatizzato, evitando i limiti della costruzione manuale di polinomi e combinazioni.

## Classificazione

Il problema della **classificazione** consiste nel **predire il valore di una variabile categorica** a partire dai valori di altre variabili. L’esempio più semplice coinvolge due variabili, $x$ e $y$, dove $x$ è la variabile in input e $y$ è quella che si vuole prevedere.

Nel caso della **classificazione binaria**, il compito è distinguere tra **due classi**. Alcuni esempi includono:

- Rilevare se un'email è spam o no
- Stabilire se un'immagine contiene un gatto
- Prevedere se una valuta salirà o scenderà
- Classificare se una casa si trova a Los Angeles o ad Albuquerque

## Predizione nella classificazione binaria

Un classificatore binario funziona stimando una **probabilità** per una delle due classi, e produce un output binario:

- $1$ se il punteggio di probabilità è **maggiore o uguale a 0.5**    
- $0$ se il punteggio è **minore di 0.5**

## Separazione dello spazio

Visivamente, può essere semplice separare con una linea retta (separazione lineare) le case di Los Angeles da quelle di Albuquerque.

![[Pasted image 20250430192132.png|500]]

Il problema è come **apprendere automaticamente una funzione che separi lo spazio**.

Nel caso della classificazione, l’ipotesi è una funzione che stima **una probabilità**. Prende in input le variabili (es. dimensione e prezzo di una casa) e restituisce un valore compreso tra **0 e 1**.

Si potrebbe pensare di usare la regressione lineare, poiché:

- L’output è numerico: $0$ o $1$
- La linea di separazione è retta

Tuttavia, la regressione lineare **non è adatta** perché non vincola l’output tra 0 e 1. Potremmo ottenere valori maggiori di 1 o minori di 0 con dati non visti.


## Funzione logistica (sigmoide)

Per risolvere questo problema, si può:

1. Usare una **funzione lineare** per combinare tutte le variabili in un unico valore.    
2. Applicare una **funzione che limiti l’output** tra 0 e 1.

Questa funzione è la **sigmoide**, un caso particolare della **funzione logistica**, definita su tutto $\mathbb{R}$ e con valori compresi tra 0 e 1 in modo monotonicamente crescente.

$$\text{sig}(t)=\frac{1}{1+e^{-t}}$$

## Regressione logistica

Nonostante il nome, la **regressione logistica** è un metodo di **classificazione**. La funzione ipotesi è la **composizione della sigmoide con una combinazione lineare** delle variabili di input. Se nella regressione lineare l’ipotesi era:

$h_{\text{linear}}(x_1, ..., x_n) = w_1 x_1 + \dots + w_n x_n + b$

nella regressione logistica diventa:

$h_{\text{logistic}}(x_1, ..., x_n) = \text{sigmoid}(w_1 x_1 + \dots + w_n x_n + b)$

Questa funzione restituisce valori tra 0 e 1, interpretabili come probabilità.

## Funzione di perdita per la regressione logistica

Anche in questo caso serve una **funzione di perdita** per aggiornare i pesi verso una soluzione ottimale. Consideriamo due casi, in base al valore reale $y$:

- Se $y = 0$: l’ipotesi dovrebbe restituire 0. Ogni valore maggiore rappresenta una perdita, quindi: $\text{loss}(x) = h_{\text{logistic}}(x)$
- Se $y = 1$: l’ipotesi dovrebbe restituire 1. Ogni valore inferiore rappresenta una perdita, quindi: $\text{loss}(x) = 1 - h_{\text{logistic}}(x)$

## Il problema dell’overfitting

Applicando il trucco di introdurre nuove feature (es. $x_1^2$, $x_2^3$, combinazioni complesse), si può costruire un modello molto complesso e non lineare, capace di adattarsi **perfettamente** al training set.

![[Pasted image 20250430192337.png|500]]

Ma questo non è sempre un bene: il modello rischia di essere **troppo specializzato**, e con i dati nuovi può fare errori casuali. Questo fenomeno è noto come **overfitting**. Un buon modello dovrebbe invece essere **robusto e generalizzare bene** su dati mai visti (test set).

## Soluzione: regolarizzazione

La **regolarizzazione** è una tecnica molto efficace per evitare l’overfitting. Consiste nel **modificare la funzione di perdita** aggiungendo un termine che penalizza **i pesi troppo grandi**.

![[Pasted image 20250430192454.png|600]]

Questo termine aggiuntivo cresce all’aumentare del modulo dei pesi $w$. L’effetto della regolarizzazione è:

- **Evitare pesi estremamente alti**
- **Smussare il confine di decisione**, rendendolo meno sensibile alle variazioni nei dati

## Classificazione multi-classe

La classificazione binaria può essere **generalizzata** a problemi con **più di due classi**. Esistono due approcci principali per adattare classificatori binari a un problema con $K$ classi:

- **OVA – One vs All (uno contro tutti)**: si allena un classificatore per ogni classe, distinguendola da tutte le altre. Questo approccio richiede **$K$ classificatori**.
- **OVO – One vs One (uno contro uno)**: si allena un classificatore per ogni coppia di classi. Questo approccio richiede **$\dfrac{K(K-1)}{2}$ classificatori**.

L'approccio OVA è in genere il più comune per la sua efficienza e semplicità.