
Il data mining è il processo computazionale di scoperta di schemi all'interno di grandi insiemi di dati, tramite metodi situati all'intersezione tra apprendimento automatico, statistica e sistemi di basi di dati. L'obiettivo è l'estrazione di schemi e conoscenza da grandi quantità di dati. Verranno ora mostrati alcuni esempi di schemi, conoscenze e grandi quantità di dati coinvolti nel data mining.

Gli schemi (pattern) sono regolarità nei dati che, approssimativamente, si ripetono per tutte le osservazioni in modo prevedibile. Esempi di schemi includono: nella maggior parte dei casi, il prezzo di una casa in euro è approssimativamente pari a 2500 volte i suoi metri quadrati; frequentemente, latte e cereali vengono acquistati insieme; è stato osservato che le persone acquistano dispositivi digitali in questo ordine: 1) computer personale, 2) fotocamera digitale, 3) scheda di memoria.

Una volta scoperto uno schema, possiamo comprendere la logica alla base di esso, in modo da poterlo descrivere e prevedere fenomeni simili. La conoscenza è una comprensione nuova di un argomento. Alcuni esempi di conoscenza scoperta includono: come vengono fissati i prezzi delle case in base ai metri quadrati; quali prodotti dovrebbero essere collocati più vicini nel supermercato; in quale ordine i dispositivi digitali dovrebbero essere pubblicizzati nel marketing mirato.

Per trovare schemi e dedurre conoscenze, è necessario partire da numerose osservazioni. Nel data mining, le osservazioni sono fornite sotto forma di dati omogenei, ad esempio: molti esempi di prezzi di case e le relative dimensioni; molti esempi di transazioni alimentari con l'elenco degli articoli acquistati; molti esempi di sequenze di acquisti nel reparto digitale di un grande magazzino. Una grande quantità di dati è necessaria per estrarre schemi o conoscenze che siano statisticamente significative. Se conosciamo solo il prezzo di tre case, ciò potrebbe essere dovuto al caso, non a una regolarità.

## Fasi del data mining

1. **Definizione del problema**: qual è l’obiettivo che si vuole raggiungere? Quale conoscenza si desidera estrarre?
2. **Identificazione dei dati richiesti**: quali dati sono necessari per perseguire l’obiettivo? In questa fase si raccolgono e comprendono i dati.
3. **Preparazione e pre-elaborazione**: si selezionano e puliscono i dati richiesti, formattandoli per essere utilizzabili da un algoritmo di apprendimento automatico (tra poco vedremo cos'è l'apprendimento automatico).
4. **Modellazione dell’ipotesi**: si selezionano algoritmi di apprendimento automatico e si ottimizzano i parametri per costruire un modello “predittivo” accurato.
5. **Addestramento e test**: si addestrano gli algoritmi usando un campione dei dati, e si testano su dati non visti.
6. **Verifica e distribuzione**: si verifica il modello finale con gli stakeholder (utenti finali), si preparano le visualizzazioni e si distribuisce il modello.    

## Machine learning

Il machine learning (apprendimento automatico) è al centro del data mining, poiché rappresenta il punto in cui avviene l’astrazione dai dati agli schemi. L’idea di base è che, dati molti esempi, una funzione con un obiettivo specifico (ad esempio, stimare il prezzo di una casa date le sue caratteristiche) venga “appresa” automaticamente dai dati.

Perché gli esseri umani non possono apprendere direttamente gli schemi dai dati? Perché i dati possono essere molto estesi (migliaia di record) e ad alta dimensionalità (molte variabili per ogni record); le relazioni non lineari tra variabili possono essere molto difficili da individuare; la codifica manuale di alcuni algoritmi può essere estremamente complessa: come si potrebbe programmare un software per rilevare gatti in un’immagine, partendo dai pixel grezzi?

Essendo basato sulla statistica, gli schemi e le conoscenze estratte nel data mining non sono quasi mai accurate al 100%. Potrebbero esserci molte variabili coinvolte che non sono presenti nei dati (ad esempio, il venditore di una casa potrebbe volerla vendere rapidamente a un prezzo più basso); i dati provenienti dal mondo reale sono affetti da rumore e casualità (ad esempio, un prezzo digitato erroneamente).

Il fondamento del machine learning è la teoria P.A.C.: **Probably Approximately Correct**.

“Probabilmente” indica che il modello del mondo reale, appreso tramite il machine learning, dovrebbe essere corretto con alta probabilità. “Approssimativamente” significa che il modello dovrebbe avere un basso errore di generalizzazione, ossia dovrebbe approssimare bene il caso generale ed essere accurato anche con dati mai visti.

## Concetti comuni del machine learning

Un **esempio** è un’osservazione, come i dati relativi a una singola casa. Le **variabili di input** sono le variabili fornite in ingresso per ciascun esempio (ad esempio, i metri quadrati). La **variabile di output** è quella che vogliamo inferire sulla base delle variabili di input (ad esempio, il prezzo della casa). L’**ipotesi** (o modello) è la funzione dalle variabili di input alla variabile di output appresa da un algoritmo di machine learning. L’**etichetta** (label) nei compiti supervisionati (che vedremo a breve) è il valore della variabile di output per una certa istanza di input; è fornita dai dati ed è considerata il valore vero. La **predizione** è il valore della variabile di output “indovinato” dalla funzione ipotesi e dovrebbe essere corretto con alta probabilità.

È importante notare che qui “predizione” non implica necessariamente un riferimento al tempo. Ogni volta che si prevede qualcosa nel futuro, si parla di previsione (forecast). Tutte le previsioni sono predizioni, ma non tutte le predizioni sono previsioni, come quando si utilizza la regressione per spiegare la relazione tra due variabili.

Una notazione comune è la seguente: con la lettera maiuscola $X$ si denota la matrice dei dati di input, con la lettera minuscola $y$ il vettore della variabile di output, con $m$ il numero di esempi, e con $n$ il numero di variabili per esempio.

## Tecniche supervisionate

Le tecniche supervisionate prevedono che un **training set** di dati venga fornito all'algoritmo di apprendimento automatico, con un'etichetta per ciascun esempio, che rappresenta il valore reale (ground truth) della variabile di output. Si parla di apprendimento supervisionato perché si indica esplicitamente all'algoritmo quale dovrebbe essere l'output per un insieme di esempi. La variabile di output è quindi parte del dataset. Un esempio classico è quello in cui viene fornito il prezzo reale di una casa insieme alle sue variabili di input.

L’apprendimento supervisionato si suddivide in due compiti principali, a seconda del tipo di variabile di output. Si parla di **regressione** quando la variabile di output è un valore numerico, come nel problema della previsione del prezzo di una casa. Si parla invece di **classificazione** quando la variabile di output è una classe. Lo scenario più semplice è quello della classificazione binaria, in cui si vuole prevedere se un esempio appartenga o meno a una certa classe (ad esempio, la casa si trova a Los Angeles? Sì o No).

## Tecniche non supervisionate

Le tecniche non supervisionate sono quelle in cui non viene fornito alcun valore per la variabile di output, oppure non esiste affatto una variabile di output. A volte non si ha nemmeno un obiettivo definito a priori e si vuole semplicemente scoprire regolarità e relazioni all'interno dei dati.

I compiti più comuni, di cui vedremo i primi due, sono i seguenti. Il **clustering** consiste nel suddividere gli esempi in gruppi distinti (cluster), in modo che gli esempi appartenenti allo stesso gruppo siano simili tra loro, mentre quelli appartenenti a gruppi diversi siano dissimili. Le **regole di associazione** permettono di scoprire relazioni interessanti tra variabili. Un esempio tipico è la regola: ${\text{cipolle, patate}} \Rightarrow {\text{hamburger}}$, osservata nelle vendite di un supermercato.

Altri compiti importanti sono il **rilevamento delle anomalie**, che consiste nell’individuare gli “outlier” in un insieme di esempi (ad esempio, trovare transazioni sospette in un registro di operazioni bancarie); i **modelli generativi**, che, dato un insieme di esempi, generano una nuova istanza “sintetica” con caratteristiche simili (come nella generazione automatica di melodie o nella scrittura automatica di poesie o libri); infine, l’**estrazione di caratteristiche**, che riduce il numero di variabili generando nuove caratteristiche più rappresentative.

## Interdisciplinarità

Uno dei motivi del successo del data mining e del machine learning è la possibilità di applicarli virtualmente a campi illimitati.
## Pre-elaborazione dei dati

La pre-elaborazione è una fase cruciale e trasversale a tutti i progetti di data mining, indipendentemente dall’obiettivo finale. I dati del mondo reale sono spesso problematici: possono essere incompleti, rumorosi o inconsistenti, e questo compromette direttamente la qualità dei risultati. La famosa espressione _“garbage in, garbage out”_ è particolarmente calzante in questo contesto.

I dati sono detti **incompleti** quando mancano valori di attributi, quando certi attributi d’interesse non sono presenti oppure quando le informazioni sono aggregate. Ad esempio: `occupazione = ""`. Sono **rumorosi** quando contengono errori o valori anomali, come `stipendio = -100`. Sono **inconsistenti** quando mostrano discrepanze logiche, ad esempio: `Età = 30`, `Data di nascita = 03/07/2001`, oppure `Sesso = Maschio`, `Incinta = Sì`.

## Principali fasi della pre-elaborazione

Le operazioni principali nella pre-elaborazione dei dati includono:

- **Pulizia dei dati**: consiste nel riempire valori mancanti, attenuare il rumore, individuare e trattare outlier, correggere le inconsistenze.
- **Integrazione dei dati**: unisce dati provenienti da diverse fonti (database, file, cubi) in un insieme coerente.
- **Trasformazione dei dati**: converte testo o categorie in vettori numerici (es. _bag of words_, _one-hot encoding_) per l’uso con algoritmi di apprendimento automatico.
- **Normalizzazione**: ridimensiona i valori molto grandi o molto piccoli affinché gli algoritmi di ottimizzazione possano convergere in tempi ragionevoli.

## Pulizia dei dati

Questa fase è essenziale anche nei sistemi di data warehousing, dove viene spesso considerata il problema principale. Le stesse difficoltà si riscontrano nel data mining. Gli obiettivi principali della pulizia sono:

- Riempire i valori mancanti
- Attenuare il rumore e identificare gli outlier
- Correggere le inconsistenze nei dati
- Eliminare le ridondanze dovute a integrazione di fonti diverse

## Perché i dati sono sporchi?

Le cause della presenza di dati problematici possono essere molteplici:

**Dati incompleti**:

- Valori “non applicabili” al momento della raccolta
- Differenze tra il momento della raccolta e quello dell’analisi
- Malfunzionamenti di strumenti, software o operatori umani

**Dati rumorosi**:

- Errori negli strumenti di raccolta
- Errori umani o informatici nell’inserimento
- Problemi di trasmissione dei dati

**Dati inconsistenti**:

- Provenienza da fonti eterogenee (es. diversi sistemi di coordinate geografiche)
- Violazioni di dipendenze funzionali
- Record duplicati non ancora filtrati

## Gestione dei dati mancanti

I dati non sono sempre disponibili. Ad esempio, molte osservazioni potrebbero non includere il reddito del cliente. Le cause includono guasti, dati rimossi per incoerenza, mancanze dovute a disattenzione, oppure la percezione che certi dati non fossero importanti al momento dell’inserimento.

Le tecniche per gestire i dati mancanti includono:

1. **Ignorare la tupla**: utile solo se manca l’etichetta nei compiti di classificazione. Non è efficace se le assenze sono distribuite in modo irregolare.
2. **Inserimento manuale**: molto laborioso e spesso impraticabile, specie se il valore mancante è del tutto sconosciuto.
3. **Inserimento automatico**, utilizzando:
    - Una costante globale (es. `"sconosciuto"`)
    - La media dell’attributo (es. prezzo medio delle case)
    - La media per classe (es. media dei prezzi delle case a Los Angeles)
    - Il valore più probabile, stimato tramite tecniche di inferenza come la regressione

## Gestione dei dati rumorosi

Il **rumore** nei dati rappresenta un errore casuale o una variazione indesiderata nei valori misurati. Le cause includono errori di rilevamento, inserimento o trasmissione, limiti tecnologici e convenzioni di denominazione incoerenti.

I metodi principali per trattare i dati rumorosi sono:

- **Binning**: ordinare i dati, suddividerli in intervalli (bin) e attenuarne i valori, ad esempio sostituendoli con la media del bin.
- **Regressione**: adattare i dati a una funzione matematica per eliminarne le fluttuazioni.
- **Clustering**: identificare e rimuovere gli outlier analizzando le distanze rispetto ai gruppi principali.
- **Ispezione ibrida**: combinare rilevamento automatico con controllo umano dei valori sospetti.

## Riduzione del rumore: smoothing tramite binning

Quando i bin sono stati popolati, i valori al loro interno vengono sostituiti con valori aggregati. Una sostituzione tipica è quella con la media del bin. Poiché i metodi di binning considerano solo i valori presenti all’interno dello stesso bin, si tratta di una forma di smoothing locale.

![[Pasted image 20250430184927.png|500]]


## Riduzione del rumore: regressione

La **regressione** consente di adattare i dati a una funzione continua, spesso lineare, in modo da ridurre le variazioni casuali. La regressione lineare, in particolare, consiste nel trovare la "migliore" retta che approssima la relazione tra due attributi (o variabili), in modo che uno possa essere usato per prevedere l'altro. Sebbene la regressione possa essere l'obiettivo finale di un progetto di data mining, può essere impiegata anche in fase preliminare per smussare i dati e migliorarne la qualità.

![[Pasted image 20250430184955.png|500]]

## Riduzione del rumore: clustering

Anche il **clustering** può essere utilizzato per rilevare valori anomali. I valori simili vengono raggruppati in insiemi (cluster); quelli che cadono al di fuori dei cluster possono essere considerati outlier o anomalie. Come per la regressione, il clustering può essere sia l’obiettivo finale di un progetto, sia una tecnica utile nella fase di pre-elaborazione per migliorare la qualità del dataset.

![[Pasted image 20250430185017.png|500]]

## Considerazioni sul rumore nei dati

Quando si affronta il tema del rumore nei dati, è importante considerare il compito finale del progetto, perché i requisiti cambiano a seconda del contesto.

Nel **data warehousing**, il passo successivo è il caricamento nel sistema di archiviazione. In questo caso, trattare il rumore nella fase di pre-elaborazione è fondamentale, poiché l’utente finale (il decisore) non avrà alcun modo per rimuovere o compensare il rumore nei dati.

Nel **data mining**, il passo successivo è l’addestramento di un modello di machine learning. In questo ambito, anche i modelli più semplici (come la regressione logistica) dispongono di meccanismi per gestire il rumore, come le tecniche di **regolarizzazione**, che penalizzano modelli eccessivamente complessi per evitare di adattarsi al rumore.

Un suggerimento pratico è costruire un primo modello **senza effettuare denoising**, e poi reiterare il processo di addestramento, integrando gradualmente le fasi di pre-elaborazione e pulizia.

## Integrazione dei dati

Nel data mining è spesso necessario combinare dati provenienti da più archivi, un processo noto come **integrazione dei dati**. Tuttavia, l’eterogeneità semantica e strutturale tra le diverse fonti costituisce una sfida significativa. Un problema tipico è come allineare schemi e oggetti provenienti da origini differenti. Per esempio, in una tabella A il numero identificativo del cliente può essere indicato come `A.cust_id`, mentre in una tabella B può essere `B.cust_numb`.

Questo tipo di difficoltà è noto come **Entity Identification Problem** (problema di identificazione dell’entità), che ostacola sia l’integrazione dello schema sia il riconoscimento degli oggetti tra fonti differenti.

## Il problema dell'identificazione dell'entità

Integrare schemi e confrontare oggetti tra fonti diverse può essere complesso proprio a causa del problema di identificazione dell’entità. Ad esempio, come può un analista (o un sistema automatico) essere certo che l’attributo `customer_id` in un database e `cust_number` in un altro rappresentino la stessa informazione?

## Soluzioni al problema dell’identificazione

Per affrontare il problema dell’identificazione delle entità si può fare ricorso ai **metadati degli attributi**, tra cui:

- **Il nome dell’attributo**: si possono utilizzare metriche di somiglianza tra stringhe, come la _edit distance_, per valutare quanto due nomi siano simili.
- **Il tipo di dato**: se entrambi gli attributi sono stringhe, date o numeri, è più probabile che siano confrontabili.
- **Il range dei valori**: se un attributo contiene valori nell’ordine delle migliaia e l’altro tra -1 e 1, è improbabile che rappresentino la stessa cosa.

Ad esempio, potremmo selezionare `cust_numb` come corrispondente a `cust_id` se mostra la minor distanza sintattica, lo stesso tipo di dato e un intervallo di valori simile. Tuttavia, è importante ricordare che **non è possibile determinare automaticamente queste corrispondenze con una precisione del 100%**.

## Trasformazione dei dati

Nel contesto del machine learning, ricordiamo che l’**ipotesi** è la funzione appresa dall’algoritmo, che collega le variabili di input alla variabile di output. Questa funzione è una **funzione matematica**: prende in input valori numerici e restituisce uno o più valori numerici. Di conseguenza, è necessario che tutte le variabili fornite al modello siano trasformate in forma numerica, attraverso codifiche o rappresentazioni vettoriali appropriate.

## Perché è necessaria la trasformazione dei dati

Uno dei motivi principali per cui è necessario trasformare i dati è il problema della codifica. Non tutte le variabili presenti nei dati sono numeriche. Alcune possono essere **categoriche**, come il nome di una città; altre possono essere **booleane**, come un attributo “haGarage” con valore sì/no; altre ancora possono consistere in **testo libero**, come nella descrizione “Appartamento molto carino e accogliente vicino al centro commerciale Centrum, quartiere amichevole”.

Un secondo motivo fondamentale riguarda la **scalatura** dei valori numerici. Valori numerici molto grandi possono influenzare negativamente i modelli di apprendimento automatico, che si basano su tecniche di ottimizzazione numerica. Questi algoritmi cercano di minimizzare l’errore tra l’ipotesi e i dati di addestramento, ma funzionano meglio e convergono più velocemente quando i dati numerici sono **normalizzati**, ad esempio in intervalli tra 0 e 1 oppure tra -1 e 1.

## Soluzione di codifica per il testo

Come visto nell’ambito del **recupero dell’informazione**, il testo libero può essere rappresentato come un vettore di variabili numeriche. In questa rappresentazione, chiamata **bag-of-words (BOW)**, a ogni parola del vocabolario corrisponde una variabile del vettore. Il valore di una variabile $x_i$ è pari a 1 se la $i$-esima parola del dizionario è presente nel testo, e 0 altrimenti. Questo vettore è detto **sparso**, poiché la maggior parte delle sue variabili è uguale a 0: solo le parole effettivamente presenti nel testo hanno valore 1.

Il processo di trasformazione che converte un testo libero in un vettore numerico viene chiamato **vettorializzazione**.

## Soluzione di codifica per variabili categoriche

La codifica delle variabili categoriche segue un principio simile al bag-of-words, ma con alcune differenze importanti. In questo caso, **solo una categoria può essere vera alla volta**: ad esempio, una casa non può trovarsi contemporaneamente a New York e a Miami, né può avere e non avere un garage allo stesso tempo. Inoltre, non è necessario effettuare tokenizzazione (cioè suddividere il testo in parole) né rimuovere parole non significative (_stop words_): ogni valore unico della variabile categoriale avrà la propria variabile binaria. Ad esempio, se la variabile è la città, verrà generata una variabile per ciascuna città presente nei dati.

Questa tecnica è chiamata **one-hot encoding**, perché tra le variabili generate solo una è attiva (ovvero vale 1, è “hot”), mentre tutte le altre sono impostate a 0.

## Scalatura e normalizzazione

Come verrà chiarito ulteriormente quando si approfondirà come i modelli di machine learning ottimizzano la funzione ipotesi, **la scalatura dei dati migliora le prestazioni e la precisione dei modelli**. In particolare, è importante che le variabili numeriche abbiano range coerenti per evitare che variabili con grandezza maggiore dominino l’ottimizzazione.

Il metodo più comune consiste nel normalizzare i valori all'interno dell’intervallo $[0, 1]$, applicando una **scalatura lineare** definita dalla formula:

$$\text{scale(value)} = \frac{\text{value} - \min(\text{values})}{\max(\text{values}) - \min(\text{values})}$$

Questa trasformazione rende i dati più uniformi e adatti all’elaborazione da parte degli algoritmi.

## Riepilogo

Abbiamo introdotto il processo di **Data Mining**, che consente di estrarre nuova conoscenza e schemi significativi da grandi quantità di dati. Il processo comprende diverse fasi, e finora abbiamo analizzato quella preliminare, ovvero la **pre-elaborazione dei dati**. In particolare, abbiamo visto come:

- **Pulire i dati**, rimuovendo o sostituendo valori errati o incoerenti;    
- **Ridurre il rumore**, smussando i valori rispetto a misure centrali ed eliminando gli outlier;
- **Integrare i dati**, allineando entità e attributi provenienti da fonti diverse;    
- **Trasformare i dati**, codificando testo e categorie in vettori numerici e scalando i valori in intervalli fissi.

Il risultato di questa fase è una matrice normalizzata $X$, composta da $m$ esempi e $n$ variabili, che rappresenta l’input per i successivi algoritmi di apprendimento automatico.
